{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3\n",
    "import numpy as np\n",
    "import pykitti\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from CMRNet.utils import (to_rotation_matrix, mat2xyzrpy, merge_inputs, overlay_imgs, quat2mat,\n",
    "                   quaternion_from_matrix, rotate_back, rotate_forward,\n",
    "                   tvector2mat)\n",
    "from CMRNet.models.CMRNet.CMRNet import CMRNet\n",
    "from CMRNet.camera_model import CameraModel\n",
    "from torchvision import transforms\n",
    "import visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefineEstimate(object):\n",
    "    \"\"\"\n",
    "    Functionality to refine an estimate of a pose\n",
    "    using CMR Net\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        path_to_dataset = config[\"path_to_dataset\"] \n",
    "        path_to_map = config[\"path_to_map\"]\n",
    "        weight_paths = config[\"weight_paths\"]\n",
    "        sequence = config[\"sequence\"]\n",
    "        # setup kitti manager\n",
    "        self.kitti = pykitti.odometry(path_to_dataset, sequence)\n",
    "        # load the downsampled map\n",
    "        full_map = o3.read_point_cloud(path_to_map)\n",
    "        # convert map into torch tensor. this is (3,no of points) coordinates\n",
    "        voxelized = torch.tensor(full_map.points, dtype=torch.float)\n",
    "        # added a extra homogeneous cordinate\n",
    "        voxelized = torch.cat((voxelized, torch.ones([voxelized.shape[0], 1], dtype=torch.float)), 1)\n",
    "        voxelized = voxelized.t()\n",
    "        voxelized = voxelized.to(\"cuda\")\n",
    "        # this containsthe intensities of each of this point\n",
    "        vox_intensity = torch.tensor(full_map.colors, dtype=torch.float)[:, 0:1].t()\n",
    "        velo2cam2 = torch.from_numpy(self.kitti.calib.T_cam2_velo).float().to(\"cuda\")\n",
    "        self.map = voxelized\n",
    "        self.velo2cam = velo2cam2\n",
    "        self.cam2velo = self.velo2cam.inverse()\n",
    "        self.map_intensity = vox_intensity\n",
    "        # load the models into a list\n",
    "        self.image_shape = (384,1280)\n",
    "        self.models = self.load_models(weight_paths)\n",
    "        # initialize a camera model used to project lidar point cloud\n",
    "        self.cam_params = self.get_calib_kitti(sequence)\n",
    "        self.camera_model = CameraModel(focal_length=self.cam_params[:2], \n",
    "                                        principal_point=self.cam_params[2:])\n",
    "    \n",
    "    def get_calib_kitti(self, sequence):\n",
    "        if sequence == '00':\n",
    "            return torch.tensor([718.856, 718.856, 607.1928, 185.2157])\n",
    "        elif sequence == '03':\n",
    "            return torch.tensor([721.5377, 721.5377, 609.5593, 172.854])\n",
    "        elif sequence in ['05', '06', '07', '08', '09']:\n",
    "            return torch.tensor([707.0912, 707.0912, 601.8873, 183.1104])\n",
    "        else:\n",
    "            raise TypeError(\"Sequence Not Available\")\n",
    "            \n",
    "    def load_models(self,weight_paths):\n",
    "        \"\"\"\n",
    "        Loads the models stored in the paths sent.\n",
    "        Args:\n",
    "        weight_paths: A list of paths for the models\n",
    "        Returns:\n",
    "        A list of models\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        for i, path in enumerate(weight_paths):\n",
    "            model = CMRNet(self.image_shape, use_feat_from=1, md=4, use_reflectance=False)\n",
    "            checkpoint = torch.load(path, map_location='cpu')\n",
    "            if i == 0:\n",
    "                self.occlusion_th = checkpoint['config']['occlusion_threshold']\n",
    "                self.occlusion_kernel = checkpoint['config']['occlusion_kernel']\n",
    "            saved_state_dict = checkpoint['state_dict']\n",
    "            model.load_state_dict(saved_state_dict)\n",
    "            model = model.to(\"cuda\")\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "        return models\n",
    "    \n",
    "    def project_and_crop_pc_into_cam_frame(self, camera_pose):\n",
    "        \"\"\"\n",
    "        Projects the pointcloud into camera frame. \n",
    "        Selects points within the view of the camera and returns them.\n",
    "        Args:\n",
    "        camera_pose: a list of size 8, [tx, ty, tz, qx, qy, qz, qw]\n",
    "        Returns:\n",
    "        projected_pc: a dict with points projected in camera frame and the relevant intensities\n",
    "        \"\"\"\n",
    "        T_cam = torch.tensor([float(x[1]), float(x[2]), float(x[3])])\n",
    "        R_cam = torch.tensor([float(x[7]), float(x[4]), float(x[5]), float(x[6])])\n",
    "        # compute the camera pose as a transformation matrix\n",
    "        rot_mat_cam = to_rotation_matrix(R_cam, T_cam)\n",
    "        # compute the velodyne pose as a transformation matrix from this\n",
    "        rot_mat_cam = rot_mat_cam.to(\"cuda\")\n",
    "        rot_mat_velo = torch.mm(self.cam2velo, rot_mat_cam)\n",
    "        # transform the velodyne point cloud positions into the current velo dyne pose\n",
    "        local_map = torch.mm(rot_mat_velo, local_map).t()\n",
    "        # select indices within viewing range\n",
    "        indexes = local_map[:, 1] > -25.\n",
    "        indexes = indexes & (local_map[:, 1] < 25.)\n",
    "        indexes = indexes & (local_map[:, 0] > -10.)\n",
    "        indexes = indexes & (local_map[:, 0] < 100.)\n",
    "        local_map = local_map[indexes]\n",
    "        local_intensity = local_intensity[:, indexes]\n",
    "        # convert the positions into camera frame\n",
    "        local_map = torch.mm(self.velo2cam, local_map.t())\n",
    "        local_map = local_map[[2, 0, 1, 3], :]\n",
    "        projected_pc = {\n",
    "            \"positions\": local_map,\n",
    "            \"intensities\": local_intensity\n",
    "        }\n",
    "        return projected_pc\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess image to feed into network\n",
    "        \"\"\"\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        image = to_tensor(image)\n",
    "        image = normalization(image)\n",
    "        return image\n",
    "        \n",
    "    def project_lidar(self, pc, image_shape, shape_pad):\n",
    "        \"\"\"\n",
    "        Project the lidar point cloud into an 2D array\n",
    "        \"\"\"\n",
    "        uv, depth, points, refl = self.camera_model.project_pytorch(pc, image_shape, None)\n",
    "        uv = uv.t().int()\n",
    "        depth_img = torch.zeros(real_shape[:2], device='cuda', dtype=torch.float)\n",
    "        depth_img += 1000.\n",
    "        depth_img = visibility.depth_image(uv, depth, depth_img, uv.shape[0], image_shape[1], image_shape[0])\n",
    "        depth_img[depth_img == 1000.] = 0.\n",
    "        projected_points = torch.zeros_like(depth_img, device='cuda')\n",
    "        projected_points = visibility.visibility2(depth_img, self.cam_params, projected_points, depth_img.shape[1],\n",
    "                                                      depth_img.shape[0], self.occlusion_th,\n",
    "                                                      self.occlusion_kernel)\n",
    "        projected_points /= 100.\n",
    "        projected_points = F.pad(projected_points, shape_pad)\n",
    "        projected_points = projected_points.unsqueeze(0)\n",
    "        \n",
    "    def refine_pose_estimate(self, predicted_pose, image):\n",
    "        \"\"\"\n",
    "        Get a refined estimate using CMRNet\n",
    "        \"\"\"\n",
    "        processed_image = self.preprocess_image(image)\n",
    "        image_shape = (processed_image.shape[1], processed_image.shape[2], processed_image.shape[0])\n",
    "        processed_image = processed_image.cuda()\n",
    "        shape_pad = [0, 0, 0, 0]\n",
    "        shape_pad[3] = (img_shape[0] - rgb.shape[1])\n",
    "        shape_pad[1] = (img_shape[1] - rgb.shape[2])\n",
    "        processed_image = F.pad(processed_image, shape_pad)\n",
    "        # project point cloud into 2D array\n",
    "        local_pc_in_cam_frame = self.project_and_crop_pc_into_cam_frame(predicted_pose)\n",
    "        pc_image = self.project_lidar(local_pc_in_cam_frame, image_shape, shape_pad)\n",
    "        # iteratively refine\n",
    "        RT_cumulative = torch.eye(4)\n",
    "        for i, model in enumerate(self.models):\n",
    "            T_predicted, R_predicted = model[i](processed_image, pc_image)\n",
    "            R_predicted = quat2mat(R_predicted[0])\n",
    "            T_predicted = tvector2mat(T_predicted[0])\n",
    "            RT_predicted = torch.mm(T_predicted, R_predicted)\n",
    "            RT_cumulative = torch.mm(RT_cumulative, RT_predicted)\n",
    "            local_pc_in_cam_frame = rotate_forward(local_pc_in_cam_frame, RT_predicted)\n",
    "            pc_image = self.project_lidar(local_pc_in_cam_frame, image_shape, shape_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth poses are not avaialble for sequence 00.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"weight_paths\": ['./CMRNet/checkpoints/iter1.tar','./CMRNet/checkpoints/iter2.tar','./CMRNet/checkpoints/iter3.tar'],\n",
    "    \"path_to_map\": \"./CMRNet/map-00_0.1_0-300.pcd\",\n",
    "    \"path_to_dataset\": \"./CMRNet/KITTI_ODOMETRY\",\n",
    "    \"sequence\": \"00\"\n",
    "}\n",
    "\n",
    "ref = RefineEstimate(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
